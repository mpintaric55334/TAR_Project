{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773ffacd-0d4e-4bd2-9357-6a95848d5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "from data_preprocessing.features import Features\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702dff02-5877-4b10-8878-7e6ee08be7c2",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1e3ff5-71c3-4d72-b44b-47466437e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features()\n",
    "features.extract_features(\"C:\\\\Users\\\\User\\\\Documents\\\\8.semestar\\\\APT\\\\Projekt\\\\TAR_Project\\\\data\\\\features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c125f16-09a3-4f29-8ea4-eea065bd9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features = {}\n",
    "dict_feature_num = {}\n",
    "duplicates = {}\n",
    "\n",
    "k = 1\n",
    "for i,f in zip(features.feature_idx, features.features):\n",
    "    pronaden = 0\n",
    "    for key, value in dict_features.items():\n",
    "        if value == f:\n",
    "            pronaden = 1;\n",
    "            duplicates[i] = key\n",
    "\n",
    "    if (pronaden == 0):\n",
    "        dict_features[i] = f\n",
    "        dict_feature_num[i] = k\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e979bf-1da4-4eff-a546-838e4639bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(dict_features.items(), columns=['Key', 'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663545cd-c6df-4c6b-a595-724785592450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e494047-a74d-4746-8b4a-bb39b777d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = pd.read_csv(\"../data/patient_notes.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6fa3d4-3b3b-4991-9432-1c2b26bd8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.merge(patient_df, on=['pn_num', 'case_num'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60017b49-392f-4e72-8f64-87b60b4657cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['annotation'].apply(lambda x: x != \"[]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c65e893-eec0-40e1-8bcc-85fa2b6aa022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tekst_preproccesed = []\n",
    "labels = []\n",
    "labels_binary = []\n",
    "\n",
    "for index, p in patient_df.iterrows():\n",
    "    mapa = tokenizer(p[\"pn_history\"], return_offsets_mapping=True)\n",
    "    off_map = mapa[\"offset_mapping\"][1:-1]\n",
    "    \n",
    "    tekst_p = tokenizer.tokenize(p[\"pn_history\"])\n",
    "    #tekst_p = re.split(r'[\\s,.!?;:{}()/-]+', p[\"pn_history\"].lower().strip())\n",
    "    \n",
    "    label_p = len(tekst_p) * [0]\n",
    "    label_b = len(tekst_p) * [0]\n",
    "\n",
    "    row_df = pd.DataFrame.from_dict([p])\n",
    "    feature_df = pd.merge(train_df, row_df[['case_num', 'pn_num']], on=['case_num', 'pn_num'], how='inner')\n",
    "    \n",
    "    if not feature_df.empty:\n",
    "        for locs in feature_df[\"location\"]:\n",
    "            feat_num = feature_df.loc[feature_df[\"location\"] == locs, \"feature_num\"].item()\n",
    "            if feat_num in duplicates.keys():\n",
    "                feat_num = dict_feature_num[duplicates[feat_num]]\n",
    "            else:\n",
    "                feat_num = dict_feature_num[feat_num]\n",
    "\n",
    "            locs = re.sub(r';', \"','\", locs)\n",
    "            locs = [tuple(map(int, pair.split())) for pair in ast.literal_eval(locs)]\n",
    "            for l in locs:\n",
    "                x, y = l[0], l[1]\n",
    "\n",
    "                for index, (start_i, end_i) in enumerate(off_map):\n",
    "                    if start_i >= x and y >= end_i:\n",
    "                        label_p[index] = feat_num\n",
    "                        label_b[index] = 1\n",
    "\n",
    "        tekst_preproccesed.append(tekst_p)\n",
    "        labels.append(label_p)\n",
    "        labels_binary.append(label_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ec866a1-0b43-4cda-a591-b376c1cea1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Training...\n",
      "torch.Size([2720, 132])\n",
      "torch.Size([2720])\n",
      "torch.Size([2690, 132])\n",
      "torch.Size([2690])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002475D50D570>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\miniconda3\\envs\\tar\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 783, in _clean_thread_parent_frames\n",
      "    if phase != \"start\":\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 133\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#print(targets)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#predictions = torch.nn.functional.softmax(output_scores, dim=1)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output_scores\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes), targets\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 133\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m    135\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tar\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tar\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "def encode_data(data):\n",
    "    inputs_ids = []\n",
    "    \n",
    "    for t in tekst_preproccesed:\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(t)\n",
    "        #input_ids = torch.tensor([token_ids])\n",
    "        \n",
    "        #inputs_ids.append([input_ids.squeeze()])\n",
    "        inputs_ids.append(token_ids)\n",
    "\n",
    "    return inputs_ids\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.lengths = [len(text) for text in data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = (torch.tensor(self.data[idx]), self.targets[idx], self.lengths[idx])\n",
    "        return sample\n",
    "\n",
    "def pad_collate_fn(batch, pad_index):\n",
    "    texts, labels, lengths = zip(*batch)\n",
    "\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=pad_index)\n",
    "    \n",
    "    max_length = texts.shape[1]\n",
    "    \n",
    "    padded_labels = []\n",
    "    for label, length in zip(labels, lengths):\n",
    "        padded_label = torch.cat([torch.tensor(label), torch.full((max_length - length,), 133)])\n",
    "        padded_labels.append(padded_label)\n",
    "    padded_labels = torch.stack(padded_labels)\n",
    "\n",
    "    return texts, padded_labels, torch.tensor(lengths)\n",
    "\n",
    "class ModelSeqLab(nn.Module):\n",
    "    def __init__(self, bert_model_name, hidden_dim, num_layers=2, output_size=132):\n",
    "        super(ModelSeqLab, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.pad_index = 133\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        self.translate_vec = nn.Linear(self.bert.config.hidden_size, 300) #je li bolje koristiti PCA?\n",
    "        self.lstm = nn.LSTM(300, hidden_dim, num_layers=self.num_layers, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        #self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        attention_mask = input_ids.ne(self.pad_index)\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings_org = bert_outputs.last_hidden_state\n",
    "\n",
    "        embeddings = self.translate_vec(embeddings_org)\n",
    "\n",
    "        #batch_size  = input_ids.shape[0]\n",
    "        #h_01 = torch.zeros(2 * self.num_layers, batch_size, self.hidden_size).to(input_ids.device)\n",
    "        #c_01 = Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_size)).to(input_ids.device)\n",
    "\n",
    "        packed_input = pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        output_scores = self.fc(output)\n",
    "        return output_scores\n",
    "\n",
    "\n",
    "num_classes = 132\n",
    "#num_classes = 2\n",
    "\n",
    "hidden_dim = 150\n",
    "output_dim = num_classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = ModelSeqLab('bert-base-uncased', hidden_dim, output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=133)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "pad_id = BertTokenizerFast.from_pretrained(\"bert-base-uncased\").pad_token_id\n",
    "\n",
    "processed_data = encode_data(tekst_preproccesed)\n",
    "\n",
    "custom_dataset = CustomDataset(processed_data, labels) #labels_binary\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "val_size = len(custom_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True, collate_fn=lambda batch: pad_collate_fn(batch, pad_index=pad_id))\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=20, shuffle=True, collate_fn=lambda batch: pad_collate_fn(batch, pad_index=pad_id))\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_num, (podaci, targets, lengths) in enumerate(train_dataloader):\n",
    "        podaci = podaci.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "    \n",
    "        output_scores = model(podaci, lengths)\n",
    "        print(output_scores.view(-1, num_classes).shape)\n",
    "        print(targets.view(-1).shape)\n",
    "        #print(targets)\n",
    "        \n",
    "        #predictions = torch.nn.functional.softmax(output_scores, dim=1)\n",
    "        loss = criterion(output_scores.view(-1, num_classes), targets.long().view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "model.eval()\n",
    "\n",
    "targets_all = []\n",
    "predictions_all = []\n",
    "loss_all = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (podaci, targets, lengths) in enumerate(val_dataloader):\n",
    "        podaci = podaci.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        output_scores = model(podaci, lengths)\n",
    "        loss = criterion(output_scores.view(-1, num_classes), targets.long().view(-1))\n",
    "\n",
    "        loss_all += loss\n",
    "\n",
    "        predictions = torch.argmax(output_scores.view(-1, num_classes), dim=1)\n",
    "\n",
    "        targets_all.extend(targets.view(-1).cpu().tolist())\n",
    "        predictions_all.extend(predictions.cpu().tolist())\n",
    "\n",
    "    #accuracy = accuracy_score(targets_all, predictions_all)\n",
    "    #f1 = f1_score(targets_all, predictions_all)\n",
    "    #precision, recall, f1, support = precision_recall_fscore_support(targets_all, predictions_all)\n",
    "    #conf_matrix = confusion_matrix(targets_all, predictions_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9eb7b-eb1d-4f4b-b57e-658c5b0b40f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
